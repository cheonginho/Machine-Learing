{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ihbor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ihbor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "review_data = load_files(r\"movie_review\")\n",
    "X, y = review_data.data, review_data.target\n",
    "\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "X_train= np.array(X_train).reshape((1401,1500)) \n",
    "y_train= np.array(y_train).reshape((1401,1))\n",
    "X_test = np.array(X_test).reshape((601,1500))\n",
    "y_test = np.array(y_test).reshape((601,1))\n",
    "\n",
    "#############################################################################\n",
    "def layer_structure(x, y, hidden_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x: predictors\n",
    "        y: response variable\n",
    "        hidden_size: number of neurons in the hidden layer\n",
    "        \n",
    "    Output:\n",
    "        input_size: number of predictors in input dataset\n",
    "        hidden_size: number of neurosn in hidden layer\n",
    "        output_size: number of possible prediction outputs  \n",
    "    \"\"\"\n",
    "    input_size = x.shape[0]\n",
    "    hidden_size = hidden_size\n",
    "    output_size = y.shape[0]\n",
    "    \n",
    "    return (input_size, hidden_size, output_size)\n",
    "############################################################################\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        Sizes of input, hidden, and output layer\n",
    "    Output:\n",
    "        \n",
    "    \"\"\"\n",
    "    W_1 = np.random.randn(hidden_size, input_size) * 0.001\n",
    "    b_1 = np.zeros((hidden_size, 1))\n",
    "    W_2 = np.random.randn(output_size, hidden_size) * 0.001\n",
    "    b_2 = np.zeros((output_size, 1))\n",
    "    \n",
    "    parameters = {'W1':W_1,\n",
    "                  'b1':b_1,\n",
    "                  'W2':W_2,\n",
    "                  'b2':b_2}\n",
    "    \n",
    "    return parameters\n",
    "#################################################################\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Inputs z and outputs the tanh of z\n",
    "    \"\"\"\n",
    "    \n",
    "    t = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Inputs z and outputs the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return s\n",
    "##############################################\n",
    "def forward_propagation(x, parameters):\n",
    "    \"\"\"\n",
    "     Taking the input X along with the parameters and computing the output                                             \n",
    "    \"\"\"\n",
    "    \n",
    "    #retrieving the initialized parameters from the 'parameters' dictionary\n",
    "    W_1, b_1, W_2, b_2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    \n",
    "    #computing the linear and activation part of both the hidden and the output layer\n",
    "    Z_1 = np.dot(W_1, x) + b_1\n",
    "    A_1 = sigmoid(Z_1)\n",
    "    Z_2 = np.dot(W_2, A_1) + b_2\n",
    "    A_2 = sigmoid(Z_2)\n",
    "    \n",
    "    #storing results in new dictionary called 'cache'\n",
    "    cache = {'Z1': Z_1,\n",
    "             'A1': A_1,\n",
    "             'Z2': Z_2,\n",
    "             'A2': A_2}\n",
    "    \n",
    "    return cache\n",
    "##################################################\n",
    "def compute_loss(A2, y, parameters):\n",
    "    \"\"\"\n",
    "    Computing the cross-entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    n_observations = y.shape[1]\n",
    "    \n",
    "    loss = 1/n_observations * np.sum(y*np.log(A2) + (1-y)*np.log(1-A2))\n",
    "    \n",
    "    loss = np.squeeze(loss)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def backpropagation(parameters, cache, x, y):\n",
    "    \"\"\"\n",
    "    Taking the derivatives in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    m = x.shape[1] #number of observations\n",
    "    \n",
    "    #retrieving parameters as well as calculated output from forward propagation\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    #calculating the derivatives\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = 1/m * np.dot(dZ1, x.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1):\n",
    "    \"\"\"\n",
    "    Updating the parameters after backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "#############################################\n",
    "\n",
    "def neural_net(x, y, hidden_size, iterations = 1000):\n",
    "    \"\"\"\n",
    "    Putting all functions together to form a neural network\n",
    "    \"\"\"\n",
    "    input_size = len(x)\n",
    "    print(input_size)\n",
    "    output_size = len(y)\n",
    "    print(output_size)\n",
    "    \n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    costs=[]\n",
    "    \n",
    "    for i in range(0, iterations): #gradient descent\n",
    "         \n",
    "        cache = forward_propagation(x, parameters) #forward prop\n",
    "        A2 = cache['A2']\n",
    "        \n",
    "        cost = compute_loss(A2, y, parameters) #computing the cost of our output\n",
    "        costs.append(cost)\n",
    " \n",
    "        grads = backpropagation(parameters, cache, x, y) #taking the derivatives\n",
    " \n",
    "        parameters = update_parameters(parameters, grads) #updating our parameters\n",
    "    return costs,parameters\n",
    "######################################\n",
    "costs,parameters = neural_net(X_train,y_train,100,1000)\n",
    "costs_t,parameters_t = neural_net(X_test,y_test,100,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x183e8274248>]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.plot(costs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-318-bcb824fe7ed7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "cache=forward_propagation(X_train, parameters)\n",
    "cache_t=forward_propagation(X_test, parameters_t)\n",
    "A2 = cache['A2']\n",
    "A2_t = cache_t['A2']\n",
    "y_pred_test = A2_t\n",
    "y_pred_train = A2\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_train,y_pred_train))\n",
    "print(classification_report(y_train,y_pred_train))\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred_test))\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
